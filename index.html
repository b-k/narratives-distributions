<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html> <head>
 <link rel="stylesheet" href="typical.css" type="text/css" >
     <title>A Table of Narratives and Generated Distributions</title>


<!-- LaTeX math -->
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true}});
</script>

     </head><body>
     <h1>A Table of Narratives and Generated Distributions</h1>


<P>
This project lists open-form narratives and the closed-form distributions that
approximate them. Its intent is to help you build estimable statistical models on a sound
micro-level foundation.</p>

<p>Here is a simple example of going from a real-world situation to an estimable mathematical model:</p>

<p><div class="h2d"><a name="samp"></a><h6>A sample narrative</h6><p>
        
<em>Narrative</em>: Make a large series of independent, identically distributed (iid) draws from a source.
Take the mean of those draws.</p>

<p><em>Distribution</em>: The distribution of repeated means will be a Normal distribution.</p>

<p></div></p>

<p>If you wanted to write a simulation, in which individual agents each experience some iid
shock and their mean level is measured and reported, this wonderful piece of mathematics
just saved you the trouble.</p>

<p>Now you can focus your energeies on the more novel parts of the storyline. Of course,
those too may have closed-form shortcuts that save you the trouble of writing down an
open-form simulation. Your final model may wind up being a combination of closed-form
submodels.</p>

<p>Little, if any, of this is novel, and every narrative-to-distribution should have a
reference to an existing work, (including Wikipedia, because this is uncontroversial,
textbook stuff). However, it is being presented in what seems to be a novel way, to
facilitate the development of detailed micro-level narratives using known bulding
blocks where they are available.</p>

<p>The fact that we are relying on so many existing sources means that we don't need to
provide proofs here, unless they are useful for elucidating the transformation.</p>

<p>Also, estimation is often not a trivial matter. Some of these examples may break for
small $N$. We may add these notes later, but at this stage it would be nice to just get
down as many narratives as possible, and leave the estimation details to the references.</p>

<p></P>

<P>
<h1>Aggregation</h1><p></p>

<p><h2>Coin flips and die rolls</h2><p></p>

<p><div class="h2d"><a name="bernie"></a><h6>Draws from a binary option </h6><p>
        
<em>Narrative</em>: The coin-flip: one event occurs with probability $p$.</p>

<p><em>Distribution</em>: This defines the Bernoulli Distribution, which is one with probability $p$ and zero with
probability $1-p$.</p>

<p><em>Notes</em>: The variance of a Bernoulli Distribution is $p(1-p)$.</p>

<p></div></p>

<p><div class="h2d"><a name="binom"></a><h6>Multiple draws from a binary option </h6><p>
        
<em>Narrative</em>: Draw $N$ events with probability $p$.</p>

<p><em>Distribution</em>: The Binomial$(N, p)$ Distribution.</p>

<p><em>Notes</em>: The mean is $Np$ and the variance is $Np(1-p)$.</p>

<p></div></p>

<p><div class="h2d"><a name="bernie"></a><h6>Draws from a more-than-binary option </h6><p>
        
<em>Narrative</em>: The die-roll: each observation is from a list of possible outcomes, each with its own
probability of occurring, ${\bf p} = [p_1, p_2, ..., p_k]$. Exactly one event happens each time, so 
$\sum_{i=1}^k p_i = 1$. We make $n$ draws. What is the $k$-dimensional vector of observed
outcomes?</p>

<p><em>Distribution</em>: Multinomial$(n, {\bf p})$.</p>

<p></div></p>

<p><div class="h2d"><a name="negbinom"></a><h6>Draws without replacement </h6><p>
        
<em>Narrative</em>: Start with a pool of $s$ successes and $f$ failures, so $N=s+f$, and the Bernoulli $p=s/N$.
What are the odds that we get $x$ successes from $n$ draws without replacement?</p>

<p><em>Distribution</em>: Negative binomial$(s, f, n, x)$</p>

<p></div></p>

<p><h2>Central Limit Theorems</h2><p></p>

<p><div class="h2d"><a name="clt"></a><h6>Mean of univariate iid observations </h6><p>
        
<em>Narrative</em>: Make a large series of independent, identically distributed (iid) draws from a source.
Report the mean of those draws.</p>

<p><em>Distribution</em>: As $N\to\infty$, the distribution of repeated means will be a Normal Distribution, with mean $\mu=\sum x/N$ and $\sigma = \sum (x-\mu)^2/N$. 
(<a href="#klemens:modeling">klemens:modeling</a>)</p>

<p></div></p>

<p><div class="h2d"><a name="clt"></a><h6>Product of univariate iid observations </h6><p>
        
<em>Narrative</em>: Begin with a value $x$.
Make a large series of independent, identically distributed (iid) draws from a source.
Report the product, $x\cdot d_1 \cdot d_2 ...$.</p>

<p><em>Distribution</em>: As $N\to\infty$, the distribution of products will be a Lognormal Distribution, with
mean $\mu=\ln(x) +(\sum_i \ln(d_i))/N$ and $\sigma = \sum_i (\ln(d_i)-\mu)^2/N$. That
is, the log of the products will be Normally distributed, and $\mu$ and $\sigma$
indicate the mean and standard deviation of the log.  (<a href="#klemens:modeling">klemens:modeling</a>)</p>

<p></div></p>

<p><div class="h2d"><a name="rayleigh"></a><h6>Orthogonal components are Normally distributed </h6><p>
        
<em>Narrative</em>: On one axis, we have a series of iid draws, which generates a ${\cal N}(0, \sigma)$
distribution. On an orthogonal axis, another series of iid draws also generates a ${\cal N}(0, \sigma)$ distribution.
The observed scalar output $x$ is the magnitude of the resulting vector ($x=\sqrt{d_1^2+d_2^2}$).</p>

<p><em>Distribution</em>: <a href="https://en.wikipedia.org/wiki/Rayleigh_distribution">Rayleigh</a>$(\sigma, x)$ distribution</p>

<p></div></p>

<p></P>

<P>
<h1>Wait times and frequency</h1><p></p>

<p><h2>Discrete</h2><p></p>

<p><div class="h2d"><a name="geometric"></a><h6>Wait until one success</h6><p>
        
<em>Narrative</em>: One <a href="#bernie">Bernoulli</a> (coin-flip) draw per period. What is the likelihood that the
first event will occur at period $x$? (<a href="#goswami:rao">goswami:rao, p 9</a>)</p>

<p><em>Distribution</em>: <a href="https://en.wikipedia.org/wiki/Geometric_distribution">Geometric</a>$(p, x)$</p>

<p></div></p>

<p><div class="h2d"><a name="negbinom"></a><h6>Wait until $k$ successes</h6><p>
        
<em>Narrative</em>: One <a href="#bernie">Bernoulli</a> (coin-flip) draw per period. What is the likelihood that we will have to wait
$x$ draws before we observe $k$ successes?</p>

<p><em>Distribution</em>: <a href="http://en.wikipedia.org/wiki/Negative_binomial_distribution">Negative binomial</a>$(k, p, x)$</p>

<p></div></p>

<p><div class="h2d"><a name="markovgeometric"></a><h6>Markov chain stabilization</h6><p>
        
<em>Narrative</em>: A system is represented by a column vector of states ${\bf S}$, which
changes states according to a Markov matrix $P$. What is the likelihood that it
will take $t$ steps to reach a steady state, where ${\bf S}_{t+1} = {\bf S}_t\cdot P = {\bf S}_t$?</p>

<p><em>Distribution</em>: The Markov Geometric Distribution,
$$MGD(R, Q, t)={\bf 1}'(I-tR)^{-1}(tQ){\bf 1},$$
where $Q$ is the diagonal matrix matching the diagonal of $P$, $R$ is the
off-diagonal of $P$ (so $R=P-Q$), ${\bf 1}$ is an appropriately-sized column vector of ones,
and $I$ and appropriately-sized identity matrix.
(<a href="#gani:jerwood">gani:jerwood, eqn 2.9</a>), notation via (<a href="#goswami:rao">goswami:rao, p 197</a>).</p>

<p></div></p>

<p><div class="h2d"><a name="poisson"></a><h6>Events per period</h6><p>
        
<em>Narrative</em>: Independent events (rainy day, landmine, bad data) occur at the
mean rate of $\lambda$ events per span (of time, space, et cetera). What is the
probability that there will be $t$ events in a single span?</p>

<p><em>Distribution</em>: Poisson$(\lambda, t)$</p>

<p></div></p>

<p><h2>Continuous</h2><p></p>

<p><div class="h2d"><a name="exponential"></a><h6>Wait until a success</h6><p>
        
<em>Narrative</em>: Events occur via a <a href="#poisson">Poisson</a> process ($\lambda$ events per time/space span).
What is the likelihood that the first event will occur within $x$ periods?</p>

<p><em>Distribution</em>: Exponential$(\lambda, x)$</p>

<p><em>Notes</em>: The distribution is memoryless: after the first event, the time to the next event is also Exponentially distributed.</p>

<p></div></p>

<p><div class="h2d"><a name="gamma"></a><h6>Wait until $k$ successes</h6><p>
        
<em>Narrative</em>: Events are a <a href="#poisson">Poisson</a> process: $\lambda$ events per time period. What is the
likleihood that we will observe $x$ events in a span of $k$ time units?</p>

<p>Spatial version: $\lambda$ events per spatial unit. What is the likelihood that we observe $x$ events over a distance or area of $k$ units?</p>

<p><em>Distribution</em>: Gamma$(k, \lambda, x)$</p>

<p></div></p>

<p><div class="h2d"><a name="weibull"></a><h6>Wait until a success, but time scale is not even or success odds change</h6><p>
        
<em>Narrative</em>: Events occur via a <a href="#poisson">Poisson</a>-like process ($\lambda$ events per time/space span), 
so the likelihood of observing an event within $x$ periods is Exponential$(\lambda, x)$.
But the time scale is distorted, so $y=x^{1/\gamma}$. [Note that an additive or
multiplicative distortion would still give us an Exponential Distribution.]</p>

<p>Another way to describe this is that the event rate is  changing with time. If $\gamma > 1$,
events are more likely after some time. If $\gamma < 1$, events are more likely to occur early.       
(<a href="#casella:berger">casella:berger, p 103</a>)</p>

<p><em>Distribution</em>: Weibull$(\gamma, \lambda, x)$.</p>

<p></div></p>

<p></P>

<P>
<h1>Order statistics</h1><p></p>

<p><div class="h2d"><a name="betafromuniform"></a><h6>The $N$th largest draw from a Uniform</h6><p>
        
<em>Narrative</em>: The first <em>order statistic</em> of a set of numbers $x$ is the smallest number in the
set; the second is the next-to-smallest, up to the largest order statistic, which
is $\max(x)$.</p>

<p>The $\alpha+\beta-1$ elements of $x$ are drawn from a Uniform$[0, 1]$ Distribution.</p>

<p><em>Distribution</em>: The $\alpha$th order statistic has a Beta$(\alpha,\beta)$ Distribution.</p>

<p></div></p>

<p></P>

<P>
<h1>Compounds</h1><p></p>

<p><div class="h2d"><a name="pareto"></a><h6>Exponentiation of a wait for Poisson 
        </h6><p>
        
<em>Narrative</em>: The density of events is via <a href="#poisson">Poisson</a> process, so span lenghts without events ($s$) are <a href="#exponential">Exponentially</a> distributed, with parameter $\lambda$.
What is the distribution of $\exp(s)$?
Because $s\geq 0$, $\exp(sl)$ will be $\geq 1$, so
we may need to rescale so that the distribution is bounded below by $x_m$.</p>

<p><em>Distribution</em>: Pareto$(x_m, \lambda)$</p>

<p><em>Notes</em>: As with any continuous distribution with a bounded lower support, there exists some
pivot $x%$ such that $(1-x)%$ of the density in an observed set of draws is expected to be above $x%$ (e.g., 1% of the population
holds 99% of the wealth). But given this pivot $x$, $1-x^y%$ of the density is above
$x^y%$, for all $y>0$. This property makes it popular for modeling income distributions.</p>

<p>The Pareto inherits the memoryless property from the Exponential. Given a Pareto$(x_1,
\lambda)$ distribution, the portion greater than $x_2$ has a Pareto$(x_2, \lambda)$
distribution.</p>

<p></div></p>

<p></P>

<P>
<h1>Other</h1><p></p>

<p><div class="h2d"><a name="uniform"></a><h6>Everything is equally likely</h6><p>
        
<em>Narrative</em>: We know the upper and lower bounds are $u$ and $l$, but believe that any draw within that
range is as likely as any other draw within that range.</p>

<p><em>Distribution</em>: Uniform$(l, u)$</p>

<p></div></p>

<p></P>

<P>
<h1>Other</h1><p></p>

<p><div class="h2d"><a name="uniform"></a><h6>Everything is equally likely</h6><p>
        
<em>Narrative</em>: We know the upper and lower bounds are $u$ and $l$, but believe that any draw within that
range is as likely as any other draw within that range.</p>

<p><em>Distribution</em>: Uniform$(l, u)$</p>

<p></div></p>

<p></P>
<h2>Bibliography</h2>
</p><p> [<a href="#casella:berger">casella:berger</a>] Casella, G. and R. L. Berger (1990). <em>Statistical Inference</em>. Duxbury Press.</p><p> [<a href="#gani:jerwood">gani:jerwood</a>] Gani, J. and D. Jerwood (1971). Markov chain methods in chain binomial epidemic models. <em>Biometrics</em> <em>27</em>(3), 591--603.</p><p> [<a href="#goswami:rao">goswami:rao</a>] Goswami, A. and B. Rao (2006). <em>A Course in Applied Stochastic Processes</em>. Number 40 in Texts and Readings in Mathematics. Hindustan Book  Agency.</p><p> [<a href="#klemens:modeling">klemens:modeling</a>] Klemens, B. (2008). <em>Modeling with Data: Tools and Techniques for Statistical  Computing</em>. Princeton University Press.</body></html>
